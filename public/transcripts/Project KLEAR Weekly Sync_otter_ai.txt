Unknown Speaker  0:38  
Let's wait for Anthony to join

Speaker 1  1:10  
so we did a workshop on Tuesday we got a nine participants and we show you the results

Speaker 1  1:49  
rates. So what we did, so we have three different sessions. So for the for the first session we just asked participant to come up with some ICS just bring some some things so, for example, we'd like to share lab space or on the open the fridge or like in the parking lot a tourist attraction, something like that. And then the second session, we asked them to just to go go go through all the things and to think about what could be the potential goals. And then we got, I guess, around 6959 goals in total. So we got like 16 scenes in total and round 5906. I didn't remember but some number like that. And for the last session, we asked participants to Okay, so for each still, they can just go go through all the scenes and go through some of the goals. If they feel if they find it, okay, there could be let's say the goal could be achieved with some AR visualization or AR effects. It just not be the goals and just add their own kind of what kind of AI official education they will want to achieve that goal. So, human has already logged all the data into the into this sheet. So I'm trying to code or code them but I'm not sure so, this is something I would like to discuss. So about how to code such goals and average utilizations. And so right now, say I have some my initial code here. So right now I'm just recording the goals. I haven't really dove into the AR visualization part. Oh, hi them. So we were discussing the the workshop result. Let's say for girl we could have something like task reminders that we had a let's say, we have a couple goals. I would like to remind me doing something when I pop up for example, I think one one particular example is that industrial ASpace one participant said remind me to turn off all the appliances when I when I leave that's that's one and also could be the some safety alerts. So I so for example like when I do these scenarios, like on the streets, maybe with some airphone but when there's like car approaching or some car, I Horning just like give me some like visualization to alert me that's like safety alerts. That could also something like information tracking. So that so when I'm president I'm specify when I when I was when I was working, show me like the my to do list all the time. And also, there's also a lot of example I searching for certain locations. So your question is like let's say I think I went example is that so let's say in your last year last face or like a micro kitchen, person want to look for something. But then then they visualize those those stuff. And also for information retrieval. That could be less some example is that like, when I when you're looking at like a French French restaurants menu, you probably want to see the what is at that actually, like a dish look like or what does that like French words mean? Also some other like assists, such as a suggestion, suggestion means less likely, you know, grocery store participants as I just asked. What kind of I wants to make some dishes so what ingredients do I need? And also other like a task task assistance is more like a bit different type of task assistance such as, like on a street, the person wants to visualize the bottom navigate all the way to the office have something like a navigation as I'm planning such as like Okay, so this kind of abstract so I guess it's in being a I think it's like a tourist attraction something this person says I find that best spot to take photos. I'm not sure if that's reasonable, but that's one goal here. But like a foot person personalized interactivity, I think. I think you might put something like, like a music or something. I remember audio play. Yeah, I played music to match the current scene or something. So so yeah. Basically, that's just just this kind of code is just by only by myself. It's just like some thing I want to discuss with you guys like, well, well, can we get from these results? How should we cope all of this? On the other hand? Go ahead, go ahead.

Speaker 2  7:53  
I was wondering like do we do we code like goals, like with and then separate, separate them by themes and then like, do air effects as well? Like two separate themes?

Unknown Speaker  8:06  
Yeah. I'm not sure either.

Speaker 3  8:16  
Yeah, maybe we can check which of data sets have already been published. Like to train this because if we want to do like music, and sing background matching, we probably need a lot of data to train the model.

Speaker 1  8:32  
Yeah, we don't really have to just implement all of the goals. I think we will we will we need to extract the right those are failing our scope. And then kind of but for the I guess the first question is like, how can we leverage how can we use these these results? On a workshop? That's the

Speaker 4  8:57  
question. Future, filtering them with with, so only keep the ones that you can support with AR generated contents or AR displayed content.

Unknown Speaker  9:18  
Yes, that could be one direction I think.

Speaker 2  9:30  
So there's a lot of the pathfinding ones like Wayfinding.

Speaker 2  9:41  
Maybe we could include some of those, but I don't know because like Is this our system going to support like a lot of different different

Speaker 1  9:55  
I think, I think the scope is, is quite, quite important. So I put the, this proposal here and I also discuss it with with you come who is currently a postdoc at David in Bowers. So I see a week and then prior AR work, they are required kind of kind of authoring. So it's kind of not really scalable and generalizable, which is kind of true, but but he says that sometimes it's just a trade off between generalizability versus effectiveness. So especially for for some of the let's say AR training tasks, you would definitely need to need to have some offing to because I it's some it might be hard to really automatically generate those like AR stuff, but I think for our instead of AI ours like kind of focus on those kind of lightweight AR experience. Like for focus on like, Baby I got pass just like it's not like I know so yeah, I think I think it's not like a dressing journey mutation but more like a complementary to what they have, but we just feel the gap like for those like that's a everyday kind of AR experience. We all of us can like read, adapt to different contexts, different environment and then can potentially fill that gap. And be complementary to the prior work, but I think this could be the highlight of framing of our work. But the scope is still very, very important because we can really focus on everything. And also as admin is that yeah, probably we need to further code audit results to see how, how many of them can be augmented with like, AR display, or like the artifacts

Unknown Speaker  12:26  
What do you guys think?

Speaker 1  12:37  
Yeah, I think the most important thing is starting still any concern about the high level framing of the project? Or like the motivation or anything that's I guess the most important thing

Speaker 3  12:59  
you mentioned we mentioning this rack rack. We want to assume like a artifact are needed and what do we want to do is just to find the best AR effect which match the user score, right? Yeah, and probably we can come up with some elementary AR effects from this workshop results and find a way to combine them together to modify this AR, elementary AR effects and but more suitable for this specific course. I'm sure I'm not sure. Because in the in the diffusion model like nerve diffusion model, we have we have some like generate some like basic object and we can add some additional effects upon those. Maybe we can refer to this idea. We can have an add elementary effects and find a way to compose them.

Speaker 1  14:07  
Yeah, that'd be one way for the mutation. But I just yeah, one important thing is I got I just wanted to know, it's just because we are trying to convince enemy about like the motivation. So just wondering, this the the resolve for a workshop kind of help you to kind of get a kind of motivation

Speaker 4  14:37  
to do the more filtering as I suggested, I guess, and then maybe reduce the categories to four to six. Now they're a little too many categories.

Speaker 1  14:54  
Yeah. Definitely the carrier can be definitely further future.

Speaker 4  15:01  
And some of those timeouts, I'm not sure you can support it, maybe, like fighting an object, right? It's not occluded, maybe you can use a segment anything to find it. I am curious what are some of those AR supportive tasks impossible to implement without very good sensing solutions? Like detecting Floyd Yes.

Speaker 1  15:32  
Yes, that's true. So yeah, I guess Yeah, I guess our next actionable items is to really look look into the Oracles and also the AI visualization, like in the workshop, and then to do to kind of find those can be supported by AR and then filter out like a categories of either the goals and the artifacts.

Unknown Speaker  16:12  
But at least we have something to work on

Speaker 4  16:21  
and how do you define the scope? If some task is not supportive, subsidized, affordable, some supportive then you cannot let us just say anything arbitrary and they expect the system to work right? You need to tell them yes. Yeah. What's the scope?

Unknown Speaker  16:46  
Yes, that's true.

Speaker 4  16:58  
What's the another way to ask is what's the fallback if the uses says something, but when I post support what's the backup solution?

Speaker 1  17:10  
Yeah. Yeah, yeah, probably we could have better kind of finding after we do the filtering, but I think one way is, we can potentially identify what could be supported by AR and what cannot. I guess that's also very important to understand the capability of our system.

Speaker 1  17:42  
And also in terms of scope, there should also be some discussion about whether at the current stage we want to implement the system in like, let's say like a real time to send or like kind of simplify Hario. The simpler simplified scenario means is we do some something similar to like the thematic adapt paper this reconstruct the environment first, either in nerf or like 3d scanning. And then we we, we use the we asked price continues I just specify goals and then do the support those with a yarn, but now we're kind of like cannot really that's not really interactive. It's just like, when like this just like he won't queue anyway, won't answer it's not like cannot really like feedback from the users interaction then do the follow follow up. But that is something else something we want. We will need to discuss in in future future.

Speaker 1  18:59  
Okay, so let's do the filtering. And then to see if we can get better filter results. Any other questions?

Speaker 4  19:19  
Yeah, well, another one. What's the wow moment in this project? What is that example to really impress the reviewers if he will show like 10 five examples. They are mediocre really impressive, and it's not going to give us very good chance of getting paper accepted to Yeah, that moment, something that really stands out. That example kind of sends its own

Speaker 1  19:54  
hidden I think we could probably after the filming, there may be a chance that we can find some good good examples, but as as we discussed, so we're focusing on kind of more like a lightweight AR experience like for those like everyday tasks. So it might not be really complex one. But what I can imagine like if we can, let's say you have like a super demo consists of like a unit cooking process, let's say in terms of like, ordering, maybe into like from the grocery shopping, and then go going back to home and then do the cooking and do all the visualization probably would cover all the final categories. We we coded in the end have like a super demo that that could make that that may be our Eichenwald example but I'm not sure at this moment. But yeah, I think the wall example is not like a one specific. Like example it should be a series of different examples. Maybe covering different categories

Speaker 4  21:23  
and also, you need to build us kind of a proof of concept system, right that consists of the maybe, depending on what AR technology you choose, also, how many IoT devices you want to include. It needs this kind of a demo purpose system. Whereas your main conclusion is the just the intellectual, the their actual part, maybe a mapping from natural language to the Control and Display of AR contents or the control of IoT devices. That they almost like an algorithm

Speaker 1  22:23  
Yeah, so I ever had, so I discussed with ECAM and I think you kind of has pitched this idea to David. Probably, I would see if I if I can arrange a meeting with David to see if, because they're more like a more have more expertise in this type of AR world. Your new insights from them. So

Speaker 4  22:53  
yeah, it's important having some AI experts Yeah.

Unknown Speaker  23:05  
So, any other questions

Unknown Speaker  23:12  
you just don't know.

Speaker 2  23:13  
Like, if supporting, like all these different examples. It's like really that motivating I guess we'll see. After

Unknown Speaker  23:30  
What do you mean?

Speaker 2  23:31  
Yeah, because like, I don't know, because like, I feel like we should just focus on like, one niche, like on like art or accessibility. I feel like if we generalize it too broadly, I don't know if it's too convincing. For people.

Speaker 1  23:49  
I'm not sure about that right now. Yeah. We're not sure about the sockets capabilities, and we're not sure about the overall scope yet. So if, if we focus on like a specific, either art or accessibility, we're kind of narrowing our scope to specific like a domain, but that will also require us to have more like other other kind of stuff in that specific domain. That's is I guess it's depend on our focus. For me, we need I need to discuss with like, maybe you can save it on that as well.

Speaker 3  24:38  
Maybe one of the highlights during the voice control, increase the clarity of voice can show you the AR 32.

Unknown Speaker  24:54  
For example, use of voice control to generate some AR effect or tag some places or generate some AR bubble something.

Unknown Speaker  25:30  
I guess that's it for today. Okay. Well thank you Thank you.

Transcribed by https://otter.ai
